---
title: "对比学习论文串烧"
date: 2024-02-16
permalink: /posts/2024/02/对比学习论文串烧/
tags:
  - 论文阅读
  - 对比学习
category: tech
# 关键点: 开启 LaTeX 数学公式支持
mathjax: true
---

# 对比学习论文串烧

**本质思想:** 利用数据对 (positive/negative pairs) 进行无监督/自监督学习, 在 feature space 上, 将同个类别的数据 (positive) 拉近, 不属于同个类别 (negative) 的推远.

**自监督:** 通过设计代理任务 (Auxiliary Task) 产生监督信号, 进而指导模型学习. 

- 代理任务: 旨在**帮助模型学习到更良好表征** (representation) 的**不被重点关注**的任务.

综上, 对比学习需要解决的问题也就很清晰了:

- 如何设计代理任务?
- 如何构造正负数据对? 即: 如何定义正数据对? 如何定义负数据对?
- 如何学习? 即: Contrastive Loss 实现 “拉近推远” 的效果? 更进一步, 如何设计 Contrastive Loss?

接下来笔者将对比学习领域的经典论文进行笔记整理, 其中知识点主要来自b站视频 [对比学习论文讲解1](https://www.bilibili.com/video/BV1C3411s7t9/?spm_id_from=333.1007.top_right_bar_window_history.content.click&vd_source=a310f211c2101503fd8e3dc63176607c), [对比学习论文讲解2](https://www.bilibili.com/video/BV1C3411s7t9/?spm_id_from=333.1007.top_right_bar_window_history.content.click&vd_source=a310f211c2101503fd8e3dc63176607c); 同时还有: 笔者对额外相关材料的查阅以及对论文原文的粗浅理解. [沐神组织的这个读论文栏目](https://space.bilibili.com/1567748478/upload/video)虽然时间已比较久远, 但是质量非常高, 是非常不错的学习材料.

<span id='sec:instdisc'></span>

## [InstDisc](https://openaccess.thecvf.com/content_cvpr_2018/papers/Wu_Unsupervised_Feature_Learning_CVPR_2018_paper.pdf)

### 如何设计代理任务? 

该篇论文首次提出了[个体判别任务](https://zhida.zhihu.com/search/3715216596552261941). 该任务的核心思想是: 将每个独立个体作为一类进行分类任务.
![InstDisc Motivation](/images/blogs/对比学习论文串烧/InstDisc_motivation.png)

如上图所示, InstDisc 作者的 insight 是: 图片本身的相似性影响了分类器预测类别概率, **分类器预测类别的概率并不依赖于标签信息.**
- 举个例子: 判断此时一张豹子照片的类别. 
  - 由于猎豹, 雪豹等豹类图片本身相似, logits 在这几类上分的概率也较高;
  - 豹子和船, 手推车, 书架不相似, 自然 logits 分的的概率均较低. 

标签依赖性剥离, 促使作者的目光自然延申至 unsupervised learning 领域, 聚焦于如何挖掘出图片本身的信息, 即输入本身的比较. 更进一步, 则是**将每个输入作为类别**进行比较, 这也是个体判别任务提出的思维链.

> 上述思维链仅是笔者对 InstDisc 工作的个人见解与整合.

现在, 我们基于 insight 推导出代理任务的形式. 接下来自然需要解决另外两个本质问题. 

### 如何构造正负数据对?

<span id="infonce_loss_construction"></span>

### 如何设计 Contrastive Loss?

鉴于传统 $\text{softmax}$ 操作:

$$
\begin{align}
P(i|\boldsymbol{v})=\frac{\exp(\boldsymbol{w}_i^{\top} \boldsymbol{v})}{\sum_{j=1}^n \exp(\boldsymbol{w}_j^{\top} \boldsymbol{v})},
\end{align}
$$

其中, $\boldsymbol{v}=f_{\theta}(x)$ 为输入 $x$ 的特征. $\boldsymbol{w}_i$ 表示第 $i$-th 类别所对应的分类器权重, 也可以理解为: 第 $i$-th 类别的原型 (prototype). 我们不难发现, 传统 $\text{softmax}$ 操作无法直接比较输入 ($x$, 也是 $\boldsymbol{v}$). 因此作者作出改进:

$$
\begin{align} \label{eq:improved_softmax}
P(i|\boldsymbol{v})=\frac{\exp(\boldsymbol{v}_i^{\top} \boldsymbol{v}/\tau)}{\sum_{j=1}^n \exp(\boldsymbol{v}_j^{\top} \boldsymbol{v}/\tau)},
\end{align}
$$

其中, $\boldsymbol{v}=f_{\theta}(x_i)$ 是 $\theta$ 在不断更新过程中, 模型 $f_{\theta}$ 输出的经过 $\ell_2$ 归一化的特征, **务必和直接从 Memory Bank 这个历史数组中得到的 $v_i$ 区分开**; 温度系数 $\tau$, 是用来调控 logits 的分布形状, 需要结合具体任务调参. 最终得到理论损失函数:

$$
\begin{align} \label{eq:ideal_loss}
J(\theta) = -\sum_{i=1}^{n} \log P(i|f_{\theta}(x_i))
\end{align}
$$

在有监督学习中, $\text{softmax}$ 操作分母项中的 $n$ 是数据集类别数. 举个例子, 在 $\text{ImageNet}$ 上进行训练时, $n=1000$. 而在个体判别任务上, $n$ 不再是 $1000$, 而是恐怖的百万数量级, 这会导致 $\text{softmax}$ 操作极其敏感以及耗时耗力. 因此 InstDisc 在此处进行改进.


- 将多分类任务转化为多个二分类任务. 首先定义二分类概率 $h(i, \boldsymbol{v}) = \frac{P(i|\boldsymbol{v})}{P(i|\boldsymbol{v})+m P_{n}(i)}$, 其中 $P_{n}(i)\sim \frac{1}{n}$ 是噪声分布概率, $m=\frac{\text{noise samples}}{\text{data samples}}$. 
  - $h(i, \boldsymbol{v})$ 理解为: $\boldsymbol{v}$ 和给定的 $x_i$ 是正样本对还是负样本对; 而不再需要去判断 "$\boldsymbol{v}$ 对应哪一个 $x_i$", 实现了从多分类 $\to$ 二分类的转换.
- 利用蒙特卡洛近似, 将 Eq. (\ref{eq:improved_softmax}) 中的分母项替换为: $\frac{n}{m}\sum_{k=1}^{m} \exp(v_{jk}^{\top} f_i / \tau)$, 其中 $f_i=f_{\theta}(x_i)$ 是第 $i$-th 图片的特征.

$$
\begin{align} \label{eq:nce_loss}
J_{NCE}(\theta) = -E_{P_d}[\log h(i, \boldsymbol{v})]- m\cdot E_{P_n}[\log (1-h(i, \boldsymbol{v}'))],
\end{align}
$$
其中, $P_d, P_n$ 分别表示 data, noise distribution.

| 符号                 | 全称                   | 通俗理解           | 对应样本                 | 目标             |
| :------------------- | :--------------------- | :----------------- | :----------------------- | :--------------- |
| $E_{P_d}$ | Expectation over Data  | **针对当前 batch 中正样本对的平均损失** | 当前图片 vs 它自己的特征 | 让相似度变**高** |
| $E_{P_n}$ | Expectation over Noise | **针对当前 batch 中负样本对的平均损失** | 当前图片 vs 随机噪声特征 | 让相似度变**低** |

> 在优化 Eq. (\ref{eq:nce_loss}) 的过程中, 模型学到的 $h(i, \boldsymbol{v})$ 会收敛到真实的数据比率. 通过数学推导 (参见 [Gutmann & Hyvärinen, 2010](https://proceedings.mlr.press/v9/gutmann10a/gutmann10a.pdf)), 可以证明：只要负样本采样足够多 (m 足够大），通过优化这个二分类损失，模型间接地学习到了原本 Softmax 中的概率分布 $P(i|\boldsymbol{v})$.

笔者在此处对 Proximal Regularization 不作探讨, 其旨在缓解由于 "正样本数量少导致优化过程梯度波动剧烈" 问题. 和 MoCo 的 momentum encoder 思想相似, 不过约束的是模型更新前后的 feature $\boldsymbol{v}$.

## [MoCo](https://openaccess.thecvf.com/content_CVPR_2020/papers/He_Momentum_Contrast_for_Unsupervised_Visual_Representation_Learning_CVPR_2020_paper.pdf)

将对比学习抽象为字典查找 (dictionary look-up):

- 正样本对：$\lbrace x_i^{\text{anch}}, x_i^{\text{pos}} \rbrace$；负样本对：$\lbrace x_i^{\text{anch}}, x_j^{\text{neg}} \rbrace,\ \forall i \neq j$.

- 将 $x_i^{\text{anch}}$ 视作查询 (query): $x^{\text{query}}$; 将正/负样本组织成为字典 (的 key): $\lbrace x_i^{\text{key}} \rbrace_{i=0}^{N}$.


| 核心假设          | 对假设的直觉                           | MoCo 做出的对应改进             |
| ----------------- | -------------------------------------- | ------------------------------- |
| 字典要大          | 足够充分的数据确保对原始类别的特征涵盖 | 使用队列存储 key fearture       |
| 编码器要一致/相近 | 保证了对比过程中的 "可比性"            | 引入动量编码器提取 key fearture |

![MoCo 流程图](/images/blogs/对比学习论文串烧/image-20260219120333622.png)

接下来顺一下思维链: 

![改进思维链](/images/blogs/对比学习论文串烧/改进的思维链.png)

接下来就是将改进塞入对比学习的框架, 即解决三种本质问题:

- **设计代理任务:** MoCo 和 [InstDisc](#sec:instdisc) 保持一致, 选择个体判别任务.

- **构造正负数据对:** 由代理任务自然可得, 正样本对是同张图片的多个变换; 负样本对则是不同图片. 在原文中还涉及到锚点 (Anchor), 其表示的是原始图像.

- **设计 Contrastive Loss:** MoCo 基于 dictionary look-up 抽象, 指出 Contrastive Loss 的设计原则即为: whose value is low when $q$ is similar to its positive key $k_+$ and dissimilar to all other keys (considered negative keys for $q$). 最终 MoCo 选择了 InfoNCE 损失函数, 如下所示.

$$
\begin{align}
\mathcal{L}_{q} = -\log \frac{\exp(q\cdot k_{+}/\tau)}{\exp(q\cdot k_{+}/\tau) + \sum_{i=1}^{K} \exp(q\cdot k_{i}/\tau)}.
\end{align}
$$

> 详细分析见 InstDisc 的 [分析](#infonce_loss_construction).

## 不定时更新中...

---

>## Reference
>
>[1] [MoCo 论文精读](https://www.bilibili.com/video/BV1C3411s7t9?spm_id_from=333.788.videopod.sections&vd_source=a310f211c2101503fd8e3dc63176607c)
>
> [2] [MoCo 原文](https://openaccess.thecvf.com/content_CVPR_2020/papers/He_Momentum_Contrast_for_Unsupervised_Visual_Representation_Learning_CVPR_2020_paper.pdf)