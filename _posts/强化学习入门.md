---
title: "强化学习入门"
date: 2024-02-16
permalink: /posts/2024/02/强化学习入门/
tags:
  - 强化学习
  - 数学
category: tech
# 关键点: 开启 LaTeX 数学公式支持
mathjax: true
---

# 前言
笔者在学习强化学习过程中的笔记整理汇总如下. 学习资料主要来自西湖大学赵世钰老师的[强化学习的数学原理](https://space.bilibili.com/2044042934/lists/2638814?type=series)系列课程, 是非常高质量的课程, 强烈推荐.

由于该篇文章主要聚焦在笔者对赵老师课程的浅薄理解, 因此内容忽略部分内容, 包括部分证明以及定义等, 感兴趣的读者可以自行进行阅读.

# 目录
- [贝尔曼公式以及贝尔曼最优公式](#sec:bellman_equation)
- [值迭代方法](#sec:)
- [蒙特卡洛方法](#sec:)
- [RM 算法以及随机梯度下降](#sec:)
- [时序差分](#sec:)
- [值函数近似](#sec:)
- [策略梯度](#sec:)
- [Actor-Critic 方法](#sec:)

## Notation

我们站在马尔科夫决策过程 (Markov Decision Process) 的框架下, 给出下述定义以及性质. 下述概念中, 
- "Markov" 对应 "无记忆性/Memoryless property"
- "Decision" 对应 "策略/Policy"
- "Process" 理解为: 状态之间的跳转, 因此对应 "状态, 动作, 转移概率"

| 数学定义  | 直观理解 | 含义        |
| --------- |---------------------------------------------- | ----------- |
| $ s $ |  | 状态/State |
| $ S=\lbrace s_i \rbrace_{i=1}^{N_{s}} $ |  | 状态空间/State Space |
| $a$ |  | 动作/Action |
| $ A(s_i)=\lbrace a_i \rbrace_{i=1}^{N_{a}} $ | 在状态 $s_i$ 下有 $N_a$ 个状态 | 状态 $s_i$ 的动作空间/Action Space of State |
| $p(s_{next}\|s_{current}, a_i) \in [0, 1]$ | Agent 在当前状态, 采取动作 $a_i$ 会到状态 $s_{next}$ 的概率 | State transition probability |
| $\pi(a_i\|s_{current}) \in [0, 1], \forall i \in [1, N_a]$ | Agent 在当前状态, 采取动作 $a_i$ 的概率 | 策略/Policy |
| $p(r=?\|s_{current}, a_{current}) \in [0, 1]$ | Agent 选择一个动作之后得到的一个人为设定的实数 "?" 的概率 | 奖励/Reward |
| $s_{t_1}\xrightarrow[r_{t_1}]{a_{t_1}}\dots s_{t_n}\xrightarrow[r_{t_n}]{a_{t_n}} s_{target}$ | Agent 实现目标的一系列 $\lbrace state, action \rbrace$ pair | 路径/尝试/Trajectory/trail/episode |
| $\sum_{i=1}^{n} r_{t_i}$ | Agent 沿着 **一条 trajectory** 得到的 reward 总和 | Return |
| $\sum_{i=1}^{n} \gamma^{i-1} r_{t_i}$ | 逐渐衰减的 return; 控制 $\gamma$ 能够控制 policy 的远/近视 | Discounted Return |

无记忆性/Memoryless property
$$
\begin{align}
p(s_{t+1}|a_{t+1}, s_{t} \dots, a_1, s_0) = p(s_{t+1}|a_{t+1}, s_{t})
\\
p(r_{t+1}|a_{t+1}, s_{t} \dots, a_1, s_0) = p(r_{t+1}|a_{t+1}, s_{t})
\end{align}
$$

## 持续更新中...