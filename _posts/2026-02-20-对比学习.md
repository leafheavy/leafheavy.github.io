---
title: "对比学习论文串烧"
date: 2024-02-16
permalink: /posts/2024/02/对比学习论文串烧/
tags:
  - 论文阅读
  - 对比学习
category: tech
# 关键点: 开启 LaTeX 数学公式支持
mathjax: true
---

# 对比学习论文串烧

**本质思想:** 利用数据对 (positive/negative pairs) 进行无监督/自监督学习, 在 feature space 上, 将同个类别的数据 (positive) 拉近, 不属于同个类别 (negative) 的推远.

**自监督:** 通过设计代理任务 (Auxiliary Task) 产生监督信号, 进而指导模型学习. 

- 代理任务: 旨在**帮助模型学习到更良好表征** (representation) 的**不被重点关注**的任务.

综上, 对比学习需要解决的问题也就很清晰了:

- 如何设计代理任务?
- 如何构造正负数据对? 即: 如何定义正数据对? 如何定义负数据对?
- 如何学习? 即: Contrastive Loss 实现 “拉近推远” 的效果? 更进一步, 如何设计 Contrastive Loss?

## [MoCo](https://openaccess.thecvf.com/content_CVPR_2020/papers/He_Momentum_Contrast_for_Unsupervised_Visual_Representation_Learning_CVPR_2020_paper.pdf)

将对比学习抽象为字典查找 (dictionary look-up):

\begin{itemize}
    \item 正样本对 $\{x^{\text{anch}}_{i}, x^{\text{pos}}_i\}$ 中, 负样本对 $\{x^{\text{anch}}_{i}, x^{\text{neg}}_j\}\, \forall i \neq j$.
    \item 将 $x^{\text{anch}}_{i}$ 视作查询 (query): $x^{\text{query}}$; 将正/负样本组织成为字典 (的 key): $\{x_i^{\text{key}} \}_{i=0}^{N}$.
\end{itemize}

| 核心假设          | 对假设的直觉                           | MoCo 做出的对应改进             |
| ----------------- | -------------------------------------- | ------------------------------- |
| 字典要大          | 足够充分的数据确保对原始类别的特征涵盖 | 使用队列存储 key fearture       |
| 编码器要一致/相近 | 保证了对比过程中的 "可比性"            | 引入动量编码器提取 key fearture |

![MoCo 流程图](/images/blogs/对比学习论文串烧/image-20260219120333622.png)

接下来顺一下思维链: 

![改进思维链](/images/blogs/对比学习论文串烧/改进的思维链.png)

接下来就是将改进塞入对比学习的框架, 即解决三种本质问题:

\begin{itemize}
    \item \textbf{设计代理任务:} MoCo 选择[个体判别任务](https://zhida.zhihu.com/search/3715216596552261941). 该任务的核心思想是: 将每个独立个体作为一类进行分类任务.
    \item \textbf{构造正负数据对:} 由代理任务自然可得, 正样本对是同张图片的多个变换; 负样本对则是不同图片. 在原文中还涉及到锚点 (Anchor), 其表示的是原始图像.
    \item \textbf{设计 Contrastive Loss:} MoCo 基于 dictionary look-up 抽象, 指出 Contrastive Loss 的设计原则即为: whose value is low when $q$ is similar to its positive key $k_+$ and dissimilar to all other keys (considered negative keys for $q$). 最终 MoCo 选择了 InfoNCE 损失函数, 如下所示.
\end{itemize}

$$
\begin{align} \label{eq:infonce_loss}
\mathcal{L}_{q} = -\log \frac{\exp(q\cdot k_{+}/\tau)}{\exp(q\cdot k_{+}/\tau) + \sum_{i=1}^{K} \exp(q\cdot k_{i}/\tau)}.
\end{align}
$$

> 我们首先看被 $\log$ 所包裹的部分, 是不是很熟悉? 其正是 $\text{softmax}$ 操作 $\frac{\exp(z_{+})}{\exp(z_{+}) + \sum_{i=1}^{K} \exp(z_{i})}$.
>
> 在有监督学习中, $\text{softmax}$ 操作分母项中的 $K$ 是数据集类别数. 举个例子, 在 $\text{ImageNet}$ 上进行训练时, $K=1000$. 而在个体判别任务上, $K$ 不再是 $1000$, 而是恐怖的百万数量级, 这会导致 $\text{softmax}$ 操作极其敏感以及耗时耗力. 因此这里的 $K$ 我们是采样得到的负样本数量.
>
> 至于 Eq. (\ref{eq:infonce_loss}) 中存在的温度系数 $\tau$, 是用来调控 logits 的分布形状, 需要结合具体任务调参.

---

## Reference

[1] [MoCo 论文精读](https://www.bilibili.com/video/BV1C3411s7t9?spm_id_from=333.788.videopod.sections&vd_source=a310f211c2101503fd8e3dc63176607c)

[2] [MoCo 原文](https://openaccess.thecvf.com/content_CVPR_2020/papers/He_Momentum_Contrast_for_Unsupervised_Visual_Representation_Learning_CVPR_2020_paper.pdf)