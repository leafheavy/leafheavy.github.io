---
title: "熵家族v1"
date: 2024-02-16
permalink: /posts/2024/02/熵家族v1/
tags:
  - 强化学习
  - 数学
category: tech
# 关键点: 开启 LaTeX 数学公式支持
mathjax: true
---

## 前言

笔者最近在[阅读对比学习领域论文 CPC](https://leafheavy.github.io/posts/2024/02/%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0%E8%AE%BA%E6%96%87%E4%B8%B2%E7%83%A7/). 其中对 InfoNCE 损失推导涉及到了互信息, 从而涉及到信息熵. 又因为信息熵是笔者的 "老朋友": 笔者的[第一个工作](https://github.com/leafheavy/MT-TTA)的领域——测试时域适应 (Test-Time Adaptation) 中有一大类方法均是基于最小化信息熵来实现的. 

由上, 笔者希望更系统地理解信息熵, 互信息; 并将自己的学习笔记汇总整理如下. 主要学习资料来自于苏剑林的[科学空间|Scientific Spaces](https://spaces.ac.cn/). 苏神的博客质量非常高, 是非常好的学习材料. 下述笔记的逻辑模仿苏神博客内容的组织方式.

## 信息熵的演化之路

请读者转换一下视角, 把 "熵" 抛之脑后. 现在我们完全立足于数学的直觉追求, 希望构造出一个公式去刻画信息量, 称其为 "熵".

衡量信息量, 自然会出现不确定性, 因此自然引申出: 我们需要引入概率这个工具来构造 "熵". 那么它应该是概率分布 $p(x)$ 的函数, 为了研究的方便, 我们希望其是光滑函数; 此外, 它应该具备可加性. 笔者认为, 这个性质来源于信息量, 不确定性可累加的朴素直觉. 因此, 熵的形式如下: 
$$
\begin{align} \label{eq:infomation_entropy_definition}
S[p(x)]=\sum_x f(p(x)).
\end{align}
$$
接下来的问题就是, 我们应该如何确定出 $f(\cdot)$ 的形式. 

假设 $X, Y$ 是两个独立的随机变量, 对应的概率分布分别是 $p(x), p(y)$. 因为独立性, 我们直觉认为: 联合分布的信息量和单独两个分布的信息量之和应该是相等的. 也就是: 
$$
\begin{align} \label{eq:infomation_equality}
S[p(x)p(y)]=S[p(x)] + S[p(y)].
\end{align}
$$
有了上述的约束, 我们已经足以确定 $f(\cdot)$ 的形式,  进而最终确定出熵的形式.

假设 $X,Y$ 分别服从二项分布 $B(p), B(q)$, 即: $X$ 的概率分布为 $p, 1-p$; $Y$ 的概率分布为 $q, 1-q$. 那么
$$
\begin{align}
S[p(x)p(y)]
&= \sum_x\sum_y f(p(x)p(y))
\\
&= f(pq) + f(p(1-q)) + f((1-p)q) + f((1-p)(1-q)),
\\
S[p(x)]+S[p(y)]&=f(p) + f(1-p) + f(q) + f(1-q).
\end{align}
$$
根据 Eq. (\ref{eq:infomation_equality}) 可得:
$$
\begin{align}
f(pq) + f(p(1-q)) + f((1-p)q) + f((1-p)(1-q)) \notag
\\=f(p) + f(1-p) + f(q) + f(1-q).
\end{align}
$$
上述关于 $f(\cdot)$ 的函数方程, 我们仅进行直觉上求解. 

> 我们发现, 左边是自变量的积的形式, 如 $pq$, 右边是单个的自变量的形式, 如 $p$, 回想数学中的概念, 我们可以想起来, 能够把乘积变为求和的运算是取对数, 所以我们不妨设 $f(x)=h(x)\log x$.

因此有:
$$
\begin{align}
等式左边 =& h(pq)\log (pq) \notag
\\ &+ h(p(1-q))\log (p(1-q)) 
\\ &+ h((1-p)q)\log ((1-p)q) 
\\ &+ h((1-p)(1-q))\log ((1-p)(1-q))
\\ =& h(pq)\log p + h(pq)\log q
\\ &+ h(p(1-q))\log (p) + h(p(1-q))\log (1-q) 
\\ &+ h((1-p)q)\log (1-p) + h((1-p)q)\log (q) 
\\ &+ h((1-p)(1-q))\log (1-p) + h((1-p)(1-q))\log (1-q),
\\
等式右边 =& h(p)\log p + h(q)\log q \notag 
\\ &+ h(1-p)\log (1-p) 
\\ &+ h(1-q)\log (1-q)
\end{align}
$$
现在把上面一坨式子中, 相同对数的项合并起来, 全移项至等号右边得到:
$$
\begin{align}
0=& [h(p)-h(pq)-h(p(1-q))]\log p
\\ &+ [h(q)-h(pq)-h((1-p)q)]\log q 
\\ &+ [h(1-p)-h((1-p)q)-h((1-p)(1-q))]\log (1-p) 
\\ &+ [h(1-q)-h(p(1-q))-h((1-p)(1-q))]\log (1-q).
\end{align}
$$
不难发现, 若 $h(\cdot)$ 是线性函数, 上述等式自动成立. 也就是说, 若 $h(\cdot)$ 是线性函数, $f(x)=h(x)\log x$ 能够满足我们需要的性质, 构造出熵! 因此我们找到了关于 $f(\cdot)$ 函数方程的一个解: $f(x)=\alpha x \log x$. 代回 Eq. (\ref{eq:infomation_entropy_definition}) 可得最终熵的表达式:
$$
\begin{align}
S[p(x)]=\sum_x \alpha p(x) \log p(x).
\end{align}
$$
接下来, 由于表达习惯, 有: 信息量越大, 熵越大. 那么自然有, 确定事件对应的熵为 $0$, 那么不确定性事件对应的熵自然应该大于 $0$. 同样用二项分布进行分析, 形式化表达为: $\alpha p\log p + \alpha (1-p)\log(1-p)>0$, 因此可得到: $\alpha>0$. $\alpha$ 具体的数值并不重要, 其符号由上已知, 这也是最终呈现在大家眼前, 信息熵的表达式的演化之路.
$$
\begin{align} \label{eq:nfomation_entropy}
S[p(x)]=-\sum_x p(x) \log p(x).
\end{align}
$$

## 联合熵与条件熵

有了上述针对边缘熵的表达式, 接下来自然推广至多元随机变量, 得到联合熵.

> 请注意虽然我们是基于二元随机变量进行推导构建出边缘熵的表达式, 但是其也可以表达: 针对一元随机变量的信息熵

$$
\begin{align}
S[p(x,y)]=-\sum_x p(x,y) \log p(x,y).
\end{align}
$$

条件熵如何理解呢? 让我们望文生义一下, 不难发现这个概念依赖于条件概率分布.

> 我们已经知道, 条件分布就是在联合分布p(x,y)的基础上, 已经知道p(x)的分布, 求X确定时, Y的分布情况。那么条件熵自然是在联合熵的基础上, 再引入X的熵, 所剩下的熵值: 

$$
\begin{align}
S(Y|X)=S[p(x,y)]−S[p(x)]
\end{align}
$$

至于课程常见的条件熵表达式, 均可以通过上述直击本质的等式推导得到. 需要利用到下述等式:

- $p(x) = \sum_y p(x, y)$

- $p(y\mid x) = \frac{p(x, y)}{p(x)}$

*Common Expression 1:* $S(Y|X)=-\sum_x \sum_y p(x, y) \log p(y\mid x)$.

证明如下:
$$
\begin{align}
S[Y\mid X]&=S[p(x,y)]-S[p(x)]
\\ &= \bigl(-\sum_x \sum_y p(x, y)\log p(x, y) \bigr) - \bigl(-\sum_x p(x)\log p(x) \bigr)
\\ &= -\sum_x \sum_y p(x, y)\log p(x, y) + \sum_x \sum_y p(x, y)\log p(x)
\\ &= -\sum_x \sum_y p(x, y) \bigl[ \log p(x, y) - \log p(x)\bigr]
\\ &= -\sum_x \sum_y p(x, y) \log \frac{p(x, y)}{p(x)}
\\ &= -\sum_x \sum_y p(x, y) \log p(y\mid x)
\end{align}
$$
*Common Expression 2:* $S(Y|X)=-\sum_x p(x) \sum_y p(y\mid x) \log p(y\mid x)$.

证明如下:
$$
\begin{align}
S[Y\mid X]&=S[p(x,y)]-S[p(x)]
\\ &= \bigl(-\sum_x \sum_y p(x, y)\log p(x, y) \bigr) - \bigl(-\sum_x p(x)\log p(x) \bigr)
\\ &= -\sum_x \sum_y p(y\mid x)p(x)\log (p(y\mid x)p(x)) + \sum_x p(x)\log p(x)
\\ &= -\sum_x \sum_y p(y\mid x)p(x)\log p(y\mid x) -\sum_x \sum_y p(y\mid x)p(x)\log p(x) + \sum_x p(x)\log p(x)
\\ &= -\sum_x \sum_y p(y\mid x)p(x)\log p(y\mid x) \underbrace{-\sum_x p(x)\log p(x) + \sum_x p(x)\log p(x)}_{=0}
\\ &= -\sum_x p(x) \sum_y p(y\mid x)\log p(y\mid x)
\end{align}
$$

## 最大熵原理

当我们想要得到一个随机事件的概率分布时, 如果没有足够的信息能够完全确定这个概率分布, 那么最为“保险”的方案是选择使得熵最大的分布。

### 哲学意味

笔者认为, 苏神对最大熵原理的哲学意义理解很有意思, 因此这里也一并摘取记录. 

> 对最大熵原理更恰当的解释是: **承认我们的无知!** 如何理解这句话跟最大熵原理的联系呢？我们已经知道, 熵是不确定性的度量, 最大熵, 意味着最大的不确定性, 那么很显然, 我们要选取熵最大的分布, 也就意味着我们选择了我们对它最无知的分布。换言之, 承认我们对问题的无知, 不要自欺欺人, 也许我们可以相互欺骗, 但是我们永远也欺骗不了大自然。我们对自然的坦诚, 而我们相信我们对自然坦诚了, 自然也不会亏待我们。另外, 承认我们的无知, 也就意味着我们假设能从中获得最大的信息量, 这在某种意义上, 也符合最经济的法则。

### 估计框架

**当没有任何先验知识情况下, 选择让熵最大的分布. 但是当存在部分先验时, 我们要好好利用起来.** 

先验知识必然是统计结果, 而数学中通常使用期望算子来刻画统计结果. 因此下面我们用期望刻画先验知识:
$$
\begin{align}
E[f(x)]=\sum_x p(x)f(x)=\tau,
\end{align}
$$

其中, $f(x)$ 是特征函数, 直观地理解为: 事物 $x$ 的某项属性、代价或测量量; $\tau$ 是“在现实世界中观察到的统计均值, 即: 先验知识.

也就是, 在满足 $k+1$ 个约束 ($k$ 个先验知识 $+$ $1$ 个通用约束) 的情况下, 最大化 Eq. (\ref{eq:nfomation_entropy}). 我们很容易想到, 可以使用拉格朗日乘子法求解这类带有约束的极值问题. 因此我们可以得到下述式子:
$$
\begin{align}
L(p(x))=-\sum_x p(x) \log p(x) - \lambda_0 \Bigl(\sum_x p(x)-1\Bigr) - \sum_{i=1}^k \lambda_i \Bigl(\sum_x p(x)f_i(x)-\tau_i \Bigr),
\end{align}
$$
其中, $\lambda_0$ 所对应的约束是通用约束, 含义是: 概率和为 $1$. 通过对 $p(x)$ 求导并令梯度为 $0$ 易得,
$$
\begin{align}
\frac{\partial L}{\partial p(x)} = -\ln p(x) - 1 - \lambda_0 - \sum_{i=1}^k \lambda_i f_i(x)=0,
\end{align}
$$
最终解得:
$$
\begin{align}
p(x) &= \exp\left( - 1 - \lambda_0 - \sum_{i=1}^k \lambda_i f_i(x) \right)
\\ &= \underbrace{\exp(-1-\lambda_0)}_{\text{与数据无关}} \cdot \exp\left( -\sum_{i=1}^k \lambda_i f_i(x) \right)
\\ &= \frac{1}{Z} \exp\left( -\sum_{i=1}^k \lambda_i f_i(x) \right).
\end{align}
$$
又因为上述求得的 $p(x)$ 满足 $\sum_x p(x)=1$, 所以有:
$$
\begin{align}
\sum_x p(x) &=\sum_x \left[ \frac{1}{Z} \exp\left( -\sum_{i=1}^k \lambda_i f_i(x) \right) \right]=1.
\end{align}
$$
因此不难得出,
$$
Z = \sum_x \exp\left( -\sum_{i=1}^k \lambda_i f_i(x) \right).
$$
所以在实际应用 (包括写代码算 $\text{Softmax}$) 时，我们**根本不需要去求 $\lambda_0$ 具体等于几**. 因为我们已经推导出 $Z$ 的显式表达式 (所有分数的指数之和), 我们直接把所有项算出来求和作为分母即可, 绕过对 $\lambda_0$ 的显式求解。

到这里, 拉格朗日乘子法的机械化操作还未完成. 我们只需要再将求解得到的 $p(x)$ 再代回 $k$ 个约束条件就可以解出各个未知的 $\lambda_i$ (遗憾的是, 对于一般的 $f_i(x)$, 上式并没有简单解, 甚至数值求解都不容易, 这使得最大熵相关的模型都比较难使用).

### $\text{Softmax}$ 操作的关联

上面笔者为什么提到 $\text{Softmax}$ 呢？因为 $\text{Softmax}$ 正是熵最大原理的直接运用! 回顾一下上述推导得到的最大熵分布:
$$
\begin{align}
p(x) &= \frac{ \exp\left( -\sum_{i=1}^k \lambda_i f_i(x) \right)}{\sum_x \exp\left( -\sum_{i=1}^k \lambda_i f_i(x) \right)}.
\end{align}
$$
为了让它更顺眼, 不妨做点参数替换.

- **替换参数:** $-\lambda_i$ 是一个待求的常数，我们替换为权重 (Weight), 记作 $w_i$.
- **理解特征:** $f_i(x)$ 就是提取出来的第 $i$ 个特征 (Feature).
- **点积形式:** $\sum_{i} w_i f_i(x)$ 其实就是权重和特征的内积 (向量相乘), 写成矩阵形式: 有常见的线性层 $W_c^{\top} \cdot \boldsymbol{a}$, 其中 $\boldsymbol{a}$ 是上一层的激活值 (activation) 输出, 即当前层的输入. **在 $\text{Softmax}$ 操作中, $W_c$ 表示的就是分类器 (classifier) 的第 $c$ 个类别的权重; $\boldsymbol{a}$ 则表示模型的 $\text{logits}$.**

经过这样简单的变身，最大熵分布就变成了:
$$
p(x) = \frac{\exp(W_c^{\top} \cdot \boldsymbol{a})}{\sum_{c\in\mathcal{Y} = \{1,2,\dots,C\}} \exp(W_c^{\top} \cdot \boldsymbol{a})}
$$
这里需要注意的是, 分母的含义是: 在固定输入条件下，对 "互斥的离散输出状态" 做归一化. 在熵最大原理中, $x$ 是状态; 不能误导成每个输入. 而在多分类任务中, 互斥的离散输出状态空间是类别空间 $\mathcal{Y} = \{1,2,\dots,C\}$, 因此 $\sum$ 只在类别维度上进行求和.

## 互信息, KL 散度的关联

不定时更新中...


---

> # Reference
>
> [1] [“熵”不起: 从熵、最大熵原理到最大熵模型（一） - 科学空间|Scientific Spaces](https://spaces.ac.cn/archives/3534)
>
> [2] [“熵”不起：从熵、最大熵原理到最大熵模型（二） - 科学空间|Scientific Spaces](https://spaces.ac.cn/archives/3552)
>
> 